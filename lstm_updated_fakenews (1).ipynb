{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1wEitrMAzssjkAjhRCwttzQtVtL34tx0F","timestamp":1731991348640}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GIA2bxk9KYgg","executionInfo":{"status":"ok","timestamp":1732112385982,"user_tz":-330,"elapsed":715632,"user":{"displayName":"Ayush Prakash Singh","userId":"12611868934383994720"}},"outputId":"ec130ae5-aa4b-4e2d-8f42-21cb6e1d3f92"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample padded output: [[ 601    1    1  266  872 1276  888 2520    0    0    0    0    0    0\n","     0    0    0    0    0    0]\n"," [ 802  460 3409  198    1    1  989  226    0    0    0    0    0    0\n","     0    0    0    0    0    0]\n"," [2137 2520    1  460 1037 3852    1  131    1 2706    1 1205    0    0\n","     0    0    0    0    0    0]\n"," [ 108  182  794    1 1862    1    1    3    1 1862 1900    2 2415    1\n","     0    0    0    0    0    0]\n"," [3853 2934 3661 3044    1    1    1    1   35   43 2934 3821    1    0\n","     0    0    0    0    0    0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 75ms/step - accuracy: 0.8182 - loss: 0.3624 - val_accuracy: 0.8993 - val_loss: 0.2261\n","Epoch 2/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 78ms/step - accuracy: 0.9276 - loss: 0.1800 - val_accuracy: 0.9090 - val_loss: 0.2084\n","Epoch 3/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 74ms/step - accuracy: 0.9439 - loss: 0.1422 - val_accuracy: 0.9131 - val_loss: 0.2125\n","Epoch 4/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 75ms/step - accuracy: 0.9574 - loss: 0.1111 - val_accuracy: 0.9087 - val_loss: 0.2715\n","Epoch 5/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 75ms/step - accuracy: 0.9676 - loss: 0.0864 - val_accuracy: 0.9093 - val_loss: 0.3215\n","Epoch 6/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 82ms/step - accuracy: 0.9733 - loss: 0.0699 - val_accuracy: 0.9057 - val_loss: 0.3817\n","Epoch 7/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 85ms/step - accuracy: 0.9804 - loss: 0.0537 - val_accuracy: 0.9020 - val_loss: 0.4380\n","Epoch 8/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 76ms/step - accuracy: 0.9821 - loss: 0.0468 - val_accuracy: 0.9017 - val_loss: 0.4534\n","Epoch 9/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 77ms/step - accuracy: 0.9856 - loss: 0.0396 - val_accuracy: 0.8998 - val_loss: 0.4540\n","Epoch 10/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 73ms/step - accuracy: 0.9879 - loss: 0.0344 - val_accuracy: 0.8996 - val_loss: 0.4612\n","\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step\n","Accuracy on testing set: 0.8995539313161713\n","Precision on testing set: 0.8948748770626161\n","Recall on testing set: 0.9002858399296394\n","\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step\n","Confusion Matrix:\n"," [[8549  907]\n"," [ 962 8189]]\n","AUC: 0.9614327352460811\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Bidirectional, Embedding, LSTM, Dropout, Dense\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score\n","\n","# Define custom Hindi stopwords\n","stopwords_hindi = [\n","    'अत', 'अपना', 'अपनी', 'अपने', 'अभी', 'अंदर', 'आदि', 'आप', 'इत्यादि', 'इन', 'इनका', 'इन्हीं', 'इन्हें', 'इन्हों',\n","    'इस', 'इसका', 'इसकी', 'इसके', 'इसमें', 'इसी', 'इसे', 'उन', 'उनका', 'उनकी', 'उनके', 'उनको', 'उन्हीं', 'उन्हें',\n","    'उन्हों', 'उस', 'उसके', 'उसी', 'उसे', 'एक', 'एवं', 'एस', 'ऐसे', 'और', 'कई', 'कर', 'करता', 'करते', 'करना',\n","    'करने', 'करें', 'कहते', 'कहा', 'का', 'काफ़ी', 'कि', 'कितना', 'किन्हें', 'किन्हों', 'किया', 'किर', 'किस',\n","    'किसी', 'किसे', 'की', 'कुछ', 'कुल', 'के', 'को', 'कोई', 'कौन', 'कौन', 'बही', 'बहुत', 'बाद', 'बाला', 'बिलकुल',\n","    'भी', 'भीतर', 'मगर', 'मानो', 'मे', 'में', 'यदि', 'यह', 'यहाँ', 'यही', 'या', 'यिह', 'ये', 'रखें', 'रहा', 'रहे',\n","    'ऱ्वासा', 'लिए', 'लिये', 'लेकिन', 'व', 'वग़ैरह', 'वर्ग', 'वह', 'वहाँ', 'वहीं', 'वाले', 'वुह', 'वे', 'सकता',\n","    'सकते', 'सबसे', 'सभी', 'साथ', 'साबुत', 'साभ', 'सारा', 'से', 'सो', 'संग', 'ही', 'हुआ', 'हुई', 'हुए', 'है',\n","    'हैं', 'हो', 'होता', 'होती', 'होते', 'होना', 'होने'\n","]\n","\n","# Load dataset\n","data = pd.read_csv('/content/randomized_combined_dataset.csv')\n","\n","# Drop null values\n","data = data.dropna()\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import pad_sequences\n","\n","# Define stopwords for both Hindi and English\n","stopwords_hindi = set([\"\n","    'अत', 'अपना', 'अपनी', 'अपने', 'अभी', 'अंदर', 'आदि', 'आप', 'इत्यादि', 'इन', 'इनका', 'इन्हीं', 'इन्हें', 'इन्हों',\n","    'इस', 'इसका', 'इसकी', 'इसके', 'इसमें', 'इसी', 'इसे', 'उन', 'उनका', 'उनकी', 'उनके', 'उनको', 'उन्हीं', 'उन्हें',\n","    'उन्हों', 'उस', 'उसके', 'उसी', 'उसे', 'एक', 'एवं', 'एस', 'ऐसे', 'और', 'कई', 'कर', 'करता', 'करते', 'करना',\n","    'करने', 'करें', 'कहते', 'कहा', 'का', 'काफ़ी', 'कि', 'कितना', 'किन्हें', 'किन्हों', 'किया', 'किर', 'किस',\n","    'किसी', 'किसे', 'की', 'कुछ', 'कुल', 'के', 'को', 'कोई', 'कौन', 'कौन', 'बही', 'बहुत', 'बाद', 'बाला', 'बिलकुल',\n","    'भी', 'भीतर', 'मगर', 'मानो', 'मे', 'में', 'यदि', 'यह', 'यहाँ', 'यही', 'या', 'यिह', 'ये', 'रखें', 'रहा', 'रहे',\n","    'ऱ्वासा', 'लिए', 'लिये', 'लेकिन', 'व', 'वग़ैरह', 'वर्ग', 'वह', 'वहाँ', 'वहीं', 'वाले', 'वुह', 'वे', 'सकता',\n","    'सकते', 'सबसे', 'सभी', 'साथ', 'साबुत', 'साभ', 'सारा', 'से', 'सो', 'संग', 'ही', 'हुआ', 'हुई', 'हुए', 'है',\n","    'हैं', 'हो', 'होता', 'होती', 'होते', 'होना', 'होने'\n","])  # Extend this list as needed\n","stopwords_english = set([\"is\", \"and\", \"the\", \"of\", \"to\", \"in\", \"it\", \"that\", \"on\", \"for\", \"with\", \"as\", \"was\"])  # Extend this list as needed\n","\n","# Splitting input and label\n","x = data['title']  # Example: [\"यह किताब अच्छी है\", \"This is a great book\"]\n","y = data['label']  # Example: [1, 0]\n","\n","# Preprocessing: Removing stopwords for mixed language dataset\n","corpus = []\n","for review in x:\n","    review = review.split()  # Tokenize into words\n","    # Remove stopwords for both languages\n","    review = [word for word in review if word not in stopwords_hindi and word.lower() not in stopwords_english]\n","    corpus.append(' '.join(review))  # Recombine into processed sentences\n","\n","# Tokenization and Padding\n","voc_size = 5000  # Vocabulary size\n","tokenizer = Tokenizer(num_words=voc_size, oov_token=\"<OOV>\")  # Tokenizer with out-of-vocabulary handling\n","tokenizer.fit_on_texts(corpus)  # Fit tokenizer on the processed corpus\n","\n","# Convert text to numerical sequences\n","sequences = tokenizer.texts_to_sequences(corpus)\n","\n","# Padding sequences to a fixed length\n","max_len = 20  # Maximum length for sequences\n","padded = pad_sequences(sequences, padding='post', maxlen=max_len)\n","\n","# The padded array contains the tokenized and padded representation of the input data\n","print(\"Sample padded output:\", padded[:5])  # Display the first 5 padded sequences\n","\n","\n","# Model Building\n","embed_dim = 40\n","model = Sequential([\n","    Embedding(voc_size, embed_dim, input_length=20),\n","    Bidirectional(LSTM(100)),\n","    Dropout(0.2),\n","    Dense(64, activation='relu'),\n","    Dropout(0.2),\n","    Dense(32, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train-Test Split\n","x = np.array(padded)\n","y = np.array(y)\n","trainX, testX, trainY, testY = train_test_split(x, y, test_size=0.3, random_state=0)\n","\n","# Training\n","history = model.fit(trainX, trainY, epochs=10, validation_data=(testX, testY), batch_size=64)\n","\n","# Predictions and Evaluation\n","pred = model.predict(testX)\n","binary_predictions = [1 if i >= 0.5 else 0 for i in pred]\n","\n","print('Accuracy on testing set:', accuracy_score(binary_predictions, testY))\n","print('Precision on testing set:', precision_score(binary_predictions, testY))\n","print('Recall on testing set:', recall_score(binary_predictions, testY))\n","\n","# Confusion Matrix and AUC\n","cm = confusion_matrix(testY, binary_predictions)\n","# Use model.predict instead of model.predict_proba\n","probs = model.predict(testX)[:, 0]  # Get the predicted probabilities\n","\n","# Calculate AUC\n","auc = roc_auc_score(testY, probs)\n","\n","print('Confusion Matrix:\\n', cm)\n","print('AUC:', auc)\n","\n"]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Bidirectional, Embedding, LSTM, Dropout, Dense\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score\n","\n","# Define custom Hindi stopwords\n","stopwords_hindi = [\n","    'अत', 'अपना', 'अपनी', 'अपने', 'अभी', 'अंदर', 'आदि', 'आप', 'इत्यादि', 'इन', 'इनका', 'इन्हीं', 'इन्हें', 'इन्हों',\n","    'इस', 'इसका', 'इसकी', 'इसके', 'इसमें', 'इसी', 'इसे', 'उन', 'उनका', 'उनकी', 'उनके', 'उनको', 'उन्हीं', 'उन्हें',\n","    'उन्हों', 'उस', 'उसके', 'उसी', 'उसे', 'एक', 'एवं', 'एस', 'ऐसे', 'और', 'कई', 'कर', 'करता', 'करते', 'करना',\n","    'करने', 'करें', 'कहते', 'कहा', 'का', 'काफ़ी', 'कि', 'कितना', 'किन्हें', 'किन्हों', 'किया', 'किर', 'किस',\n","    'किसी', 'किसे', 'की', 'कुछ', 'कुल', 'के', 'को', 'कोई', 'कौन', 'कौन', 'बही', 'बहुत', 'बाद', 'बाला', 'बिलकुल',\n","    'भी', 'भीतर', 'मगर', 'मानो', 'मे', 'में', 'यदि', 'यह', 'यहाँ', 'यही', 'या', 'यिह', 'ये', 'रखें', 'रहा', 'रहे',\n","    'ऱ्वासा', 'लिए', 'लिये', 'लेकिन', 'व', 'वग़ैरह', 'वर्ग', 'वह', 'वहाँ', 'वहीं', 'वाले', 'वुह', 'वे', 'सकता',\n","    'सकते', 'सबसे', 'सभी', 'साथ', 'साबुत', 'साभ', 'सारा', 'से', 'सो', 'संग', 'ही', 'हुआ', 'हुई', 'हुए', 'है',\n","    'हैं', 'हो', 'होता', 'होती', 'होते', 'होना', 'होने'\n","]\n","\n","# Load dataset\n","data = pd.read_csv('/content/randomized_combined_dataset.csv')\n","\n","# Drop null values\n","data = data.dropna()\n","\n","# Splitting input and label\n","x = data['title']\n","y = data['label']\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import pad_sequences\n","\n","# Define stopwords for both Hindi and English\n","stopwords_hindi = set([\n","    'अत', 'अपना', 'अपनी', 'अपने', 'अभी', 'अंदर', 'आदि', 'आप', 'इत्यादि', 'इन', 'इनका', 'इन्हीं', 'इन्हें', 'इन्हों',\n","    'इस', 'इसका', 'इसकी', 'इसके', 'इसमें', 'इसी', 'इसे', 'उन', 'उनका', 'उनकी', 'उनके', 'उनको', 'उन्हीं', 'उन्हें',\n","    'उन्हों', 'उस', 'उसके', 'उसी', 'उसे', 'एक', 'एवं', 'एस', 'ऐसे', 'और', 'कई', 'कर', 'करता', 'करते', 'करना',\n","    'करने', 'करें', 'कहते', 'कहा', 'का', 'काफ़ी', 'कि', 'कितना', 'किन्हें', 'किन्हों', 'किया', 'किर', 'किस',\n","    'किसी', 'किसे', 'की', 'कुछ', 'कुल', 'के', 'को', 'कोई', 'कौन', 'कौन', 'बही', 'बहुत', 'बाद', 'बाला', 'बिलकुल',\n","    'भी', 'भीतर', 'मगर', 'मानो', 'मे', 'में', 'यदि', 'यह', 'यहाँ', 'यही', 'या', 'यिह', 'ये', 'रखें', 'रहा', 'रहे',\n","    'ऱ्वासा', 'लिए', 'लिये', 'लेकिन', 'व', 'वग़ैरह', 'वर्ग', 'वह', 'वहाँ', 'वहीं', 'वाले', 'वुह', 'वे', 'सकता',\n","    'सकते', 'सबसे', 'सभी', 'साथ', 'साबुत', 'साभ', 'सारा', 'से', 'सो', 'संग', 'ही', 'हुआ', 'हुई', 'हुए', 'है',\n","    'हैं', 'हो', 'होता', 'होती', 'होते', 'होना', 'होने'\n","])  # Extend this list as needed\n","stopwords_english = set([\"is\", \"and\", \"the\", \"of\", \"to\", \"in\", \"it\", \"that\", \"on\", \"for\", \"with\", \"as\", \"was\"])  # Extend this list as needed\n","\n","# Splitting input and label\n","x = data['title']  # Example: [\"यह किताब अच्छी है\", \"This is a great book\"]\n","y = data['label']  # Example: [1, 0]\n","\n","# Preprocessing: Removing stopwords for mixed language dataset\n","corpus = []\n","for review in x:\n","    review = review.split()  # Tokenize into words\n","    # Remove stopwords for both languages\n","    review = [word for word in review if word not in stopwords_hindi and word.lower() not in stopwords_english]\n","    corpus.append(' '.join(review))  # Recombine into processed sentences\n","\n","# Tokenization and Padding\n","voc_size = 5000  # Vocabulary size\n","tokenizer = Tokenizer(num_words=voc_size, oov_token=\"<OOV>\")  # Tokenizer with out-of-vocabulary handling\n","tokenizer.fit_on_texts(corpus)  # Fit tokenizer on the processed corpus\n","\n","# Convert text to numerical sequences\n","sequences = tokenizer.texts_to_sequences(corpus)\n","\n","# Padding sequences to a fixed length\n","max_len = 20  # Maximum length for sequences\n","padded = pad_sequences(sequences, padding='post', maxlen=max_len)\n","\n","# The padded array contains the tokenized and padded representation of the input data\n","print(\"Sample padded output:\", padded[:5])  # Display the first 5 padded sequences\n","\n","\n","# Model Building\n","embed_dim = 40\n","model = Sequential([\n","    Embedding(voc_size, embed_dim, input_length=20),\n","    Bidirectional(LSTM(100)),\n","    Dropout(0.2),\n","    Dense(64, activation='relu'),\n","    Dropout(0.2),\n","    Dense(32, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train-Test Split\n","x = np.array(padded)\n","y = np.array(y)\n","trainX, testX, trainY, testY = train_test_split(x, y, test_size=0.3, random_state=0)\n","\n","# Training\n","history = model.fit(trainX, trainY, epochs=10, validation_data=(testX, testY), batch_size=64)\n","\n","# Predictions and Evaluation\n","pred = model.predict(testX)\n","binary_predictions = [1 if i >= 0.5 else 0 for i in pred.flatten()]\n","\n","print('Accuracy on testing set:', accuracy_score(testY, binary_predictions))\n","print('Precision on testing set:', precision_score(testY, binary_predictions))\n","print('Recall on testing set:', recall_score(testY, binary_predictions))\n","\n","# Confusion Matrix and AUC\n","cm = confusion_matrix(testY, binary_predictions)\n","probs = pred.flatten()  # Get predicted probabilities\n","auc = roc_auc_score(testY, probs)\n","\n","print('Confusion Matrix:\\n', cm)\n","print('AUC:', auc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lA5jRHk-juSL","executionInfo":{"status":"ok","timestamp":1732113511618,"user_tz":-330,"elapsed":789600,"user":{"displayName":"Ayush Prakash Singh","userId":"12611868934383994720"}},"outputId":"5862339e-fd31-4ae2-d535-28b61c469dcf"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample padded output: [[ 538    1    1  210  807 1205  823 2444    0    0    0    0    0    0\n","     0    0    0    0    0    0]\n"," [ 737  399 3332  143    1    1  922  170    0    0    0    0    0    0\n","     0    0    0    0    0    0]\n"," [2061 2444    1  399  969 3775    1   85    1 2630    1 1134    0    0\n","     0    0    0    0    0    0]\n"," [  66  131  729    1 1789    1    1    1 1789 1827 2339    1    0    0\n","     0    0    0    0    0    0]\n"," [3776 2858 3584 2967    1    1    1    1   19   22 2858 3744    1    0\n","     0    0    0    0    0    0]]\n","Epoch 1/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 80ms/step - accuracy: 0.8230 - loss: 0.3636 - val_accuracy: 0.9113 - val_loss: 0.2132\n","Epoch 2/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 88ms/step - accuracy: 0.9290 - loss: 0.1753 - val_accuracy: 0.9139 - val_loss: 0.2105\n","Epoch 3/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 77ms/step - accuracy: 0.9455 - loss: 0.1359 - val_accuracy: 0.9080 - val_loss: 0.2393\n","Epoch 4/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 79ms/step - accuracy: 0.9566 - loss: 0.1123 - val_accuracy: 0.9094 - val_loss: 0.2540\n","Epoch 5/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 75ms/step - accuracy: 0.9642 - loss: 0.0949 - val_accuracy: 0.9051 - val_loss: 0.3024\n","Epoch 6/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 74ms/step - accuracy: 0.9727 - loss: 0.0706 - val_accuracy: 0.9054 - val_loss: 0.3494\n","Epoch 7/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 82ms/step - accuracy: 0.9779 - loss: 0.0579 - val_accuracy: 0.9013 - val_loss: 0.3418\n","Epoch 8/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 81ms/step - accuracy: 0.9808 - loss: 0.0505 - val_accuracy: 0.9002 - val_loss: 0.3764\n","Epoch 9/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 77ms/step - accuracy: 0.9861 - loss: 0.0384 - val_accuracy: 0.8998 - val_loss: 0.4691\n","Epoch 10/10\n","\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 76ms/step - accuracy: 0.9870 - loss: 0.0343 - val_accuracy: 0.8954 - val_loss: 0.5260\n","\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step\n","Accuracy on testing set: 0.8954157037673993\n","Precision on testing set: 0.8782152230971129\n","Recall on testing set: 0.9141077477871271\n","Confusion Matrix:\n"," [[8296 1160]\n"," [ 786 8365]]\n","AUC: 0.9610594854223398\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Optimized Model\n","model = Sequential([\n","    Embedding(voc_size, embed_dim, input_length=15),  # Smaller embedding dimension and sequence length\n","    Bidirectional(LSTM(50)),  # Reduced LSTM units\n","    Dropout(0.2),\n","    Dense(32, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Early Stopping\n","early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n","\n","# Training\n","history = model.fit(trainX, trainY, epochs=5, validation_data=(testX, testY), batch_size=32, callbacks=[early_stopping])\n","# Predictions and Evaluation\n","pred = model.predict(testX)\n","binary_predictions = [1 if i >= 0.5 else 0 for i in pred.flatten()]\n","\n","print('Accuracy on testing set:', accuracy_score(testY, binary_predictions))\n","print('Precision on testing set:', precision_score(testY, binary_predictions))\n","print('Recall on testing set:', recall_score(testY, binary_predictions))\n","\n","# Confusion Matrix and AUC\n","cm = confusion_matrix(testY, binary_predictions)\n","probs = pred.flatten()  # Get predicted probabilities\n","auc = roc_auc_score(testY, probs)\n","\n","print('Confusion Matrix:\\n', cm)\n","print('AUC:', auc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ddsAqvz425Ii","executionInfo":{"status":"ok","timestamp":1732115087024,"user_tz":-330,"elapsed":179171,"user":{"displayName":"Ayush Prakash Singh","userId":"12611868934383994720"}},"outputId":"8e69b12b-ea53-41b8-d8bb-f25bc978c4e4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 30ms/step - accuracy: 0.8366 - loss: 0.3399 - val_accuracy: 0.9112 - val_loss: 0.2135\n","Epoch 2/5\n","\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 30ms/step - accuracy: 0.9339 - loss: 0.1632 - val_accuracy: 0.9141 - val_loss: 0.2150\n","Epoch 3/5\n","\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 30ms/step - accuracy: 0.9501 - loss: 0.1235 - val_accuracy: 0.9119 - val_loss: 0.2170\n","\u001b[1m582/582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step\n","Accuracy on testing set: 0.9111624657387005\n","Precision on testing set: 0.9105343845816908\n","Recall on testing set: 0.9086438640585728\n","Confusion Matrix:\n"," [[8639  817]\n"," [ 836 8315]]\n","AUC: 0.972527949706753\n"]}]}]}